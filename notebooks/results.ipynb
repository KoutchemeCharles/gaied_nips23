{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining results of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_dataset_names = {\"falconcode_skill\": \"falconcode_easy\", \"falconcode_lab\": \"falconcode_hard\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_paths(config_names, datasets=[\"falconcode_skill\", \"falconcode_lab\", \"singapore\"]):\n",
    "    base_path = \"TO_SPECIFY\"\n",
    "    exp_dir = \"sft/results/\"\n",
    "    full_paths = []\n",
    "    for config_name in config_names:\n",
    "        for dataset in datasets:\n",
    "            filename = f\"{dataset}_results.json\"\n",
    "            yield os.path.join(base_path, config_name, exp_dir, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_table(all_paths, scoring, scoring_name):\n",
    "    table_frame = []\n",
    "    for path in all_paths:\n",
    "        if not os.path.exists(path):\n",
    "            warn(f\"experiment in path {path} did not finnish or failed\")\n",
    "            continue\n",
    "        \n",
    "        ds_name = path.split(\"/\")[-1].replace(\"_results.json\", \"\")\n",
    "        model_name = path.split(\"/\")[-4].replace(\"_falcon\", \"\")\n",
    "        #model_name = change_model_names.get(model_name, \"\")\n",
    "        frame = pd.Series(scoring(path)).rename(model_name).to_frame().T\n",
    "        frame[\"dataset\"] = change_dataset_names.get(ds_name, ds_name)\n",
    "        table_frame.append(frame)\n",
    "\n",
    "    results_table = pd.concat(table_frame, axis=0)\n",
    "    results_table.columns = [c.replace(f\"{scoring_name}@\", \"k = \") \n",
    "                             for c in results_table.columns if c.startswith(scoring_name)] + [\"dataset\"]\n",
    "    selected_pass = [1, 5, 10]\n",
    "    selected_columns = [c for c in results_table.columns \n",
    "                        if c.startswith(\"k = \")]\n",
    "    selected_columns = [c for c in selected_columns\n",
    "                        if int(c.replace(f\"k = \", \"\")) in selected_pass]\n",
    "    results_table = results_table[selected_columns + [\"dataset\"]]\n",
    "    \n",
    "    # change to a more appreciable format\n",
    "    results_table = (results_table\n",
    "                     .reset_index()\n",
    "                     .groupby(['index', 'dataset'])[selected_columns]\n",
    "                     .aggregate('first').unstack()\n",
    "                     .swaplevel(axis=1).sort_values(by=\"dataset\", axis=1))\n",
    "    results_table = results_table.fillna(0)\n",
    "    results_table.columns.names = ['dataset', scoring_name]\n",
    "    results_table *= 100\n",
    "    results_table = results_table.round(3)\n",
    "    \n",
    "    return results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_model_names = {\"codegen-2B-mono\": \"codegen-mono\", \"codegen-2B-multi\": \"codegen-multi\", \"codegen-350M-multi\": \"codegen-multi\", \"santacoder\": \"santacoder\",\n",
    "                      \"codegen-350M-mono\": \"codegen-mono\", \"tiny_starcoder\": \"starcoder\", \"starcoder-1B\": \"starcoder\", \"starcoder-3B\": \"starcoder\", \"codegen2-3_7B\": \"codegen2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {\"codegen-2B-mono\": \"2B\", \"codegen-2B-multi\": \"2B\", \"codegen-350M-multi\": \"350M\", \n",
    "         \"codegen-350M-mono\": \"350M\", \"tiny_starcoder\": \"164M\", \"starcoder-3B\": \"3B\", \"codegen2-3_7B\": \"3.7B\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_names = [\"tiny_starcoder_falcon\", \"codegen-350M-mono_falcon\",\n",
    "                     \"starcoder-1B_falcon\", \"codegen-2B-mono_falcon\", \n",
    "                     \"starcoder-3B_falcon\", \"codegen2-3_7B_falcon\"]\n",
    "all_paths = list(get_full_paths(experiments_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace = lambda series: pd.Series([s.replace(\"B\",\"000000\").replace(\"M\", \"00\") for s in series])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass_at_k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.files import json2data\n",
    "\n",
    "def get_pass(path):\n",
    "    return json2data(path)[\"pass_at_k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = create_result_table(all_paths, get_pass, \"pass\")\n",
    "results_table.index = pd.MultiIndex.from_tuples([(i, sizes.get(i, \"1B\")) for i in results_table.index])\n",
    "results_table.index.names = [\"model\", \"size\"]\n",
    "results_table = results_table.reset_index(level=1, drop=False)\n",
    "results_table.index = [change_model_names.get(i, i) for i in results_table.index]\n",
    "results_table = results_table.sort_values(by=\"size\", key=lambda s: replace(s).astype(int), ascending=True)\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"Pass@k for our two scenarios\"\n",
    "results_table.index.name = \"model\"\n",
    "results_table.columns.names = [\"\", \"\"]\n",
    "print(results_table.to_latex(multirow=True, multicolumn=True, longtable=False, \n",
    "                             float_format=\"%.2f\", escape=True, multicolumn_format='c', \n",
    "                             bold_rows=False, \n",
    "                             caption=caption, label=\"tab: pass_at\", position=\"htbp!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rouge_at_k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from src.utils.distance import rougelcsum_dist\n",
    "\n",
    "def compute_buggy_rouge_at_k(prob_df):\n",
    "    buggy = prob_df[\"source_code\"].iloc[0]\n",
    "    gencor = prob_df[\"generation_correct\"]\n",
    "    rouges = [rougelcsum_dist(buggy, gen, get_score=True) for gen in prob_df[\"generation\"]]\n",
    "    # we put that at 0 (minimum score) if the code is not correct\n",
    "    rouges = [r if c else 0 for r, c in zip(rouges, gencor)]\n",
    "    # we estimate rouge by taking all unique combinations\n",
    "    rouge_at_k = {f\"rouge@{k}\": np.mean([max(score) for score in combinations(rouges, k)]) \n",
    "                  for k in range(1, len(rouges) + 1)}\n",
    "\n",
    "    return pd.Series(rouge_at_k)\n",
    "  \n",
    "def get_rouge(path):\n",
    "    df = pd.DataFrame(json2data(path)[\"eval_ds\"])\n",
    "    if \"id\" not in df:\n",
    "        warn(f\"Path {path} does not have the column id, results invalid\")\n",
    "        return pd.Series()\n",
    "    \n",
    "    rouge_at = df.groupby(\"id\").apply(compute_buggy_rouge_at_k)\n",
    "    rouge_at = rouge_at.mean(axis=0)\n",
    "    return rouge_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_table = create_result_table(all_paths, get_rouge, \"rouge\")\n",
    "results_table.index = pd.MultiIndex.from_tuples([(i, sizes.get(i, \"1B\")) for i in results_table.index])\n",
    "results_table.index.names = [\"model\", \"size\"]\n",
    "results_table = results_table.reset_index(level=1, drop=False)\n",
    "results_table.index = [change_model_names.get(i, i) for i in results_table.index]\n",
    "results_table = results_table.sort_values(by=\"size\", key=lambda s: replace(s).astype(int), ascending=True)\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"Rouge results\"\n",
    "results_table.index.name = \"\"\n",
    "results_table.columns.names = [\"\", \"\"]\n",
    "print(results_table.to_latex(multirow=True, multicolumn=True, longtable=False, \n",
    "                             float_format=\"%.2f\", escape=True, multicolumn_format='c', \n",
    "                             bold_rows=False, \n",
    "                             caption=caption, label=\"tab: rouge_at\", position=\"htbp!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual analysis of model generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(json2data(all_paths[1])[\"eval_ds\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.generation_correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.generation_correct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.choice(range(len(data)))\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.source_code.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.generation.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data.generation_correct][\"source_code\"].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data.generation_correct][\"generation\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.type.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (neurips23)",
   "language": "python",
   "name": "neurips23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
